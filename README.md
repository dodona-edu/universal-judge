# TESTed: universal judge for educational software testing

TESTed is an education test framework that supports evaluating submissions in multiple programmes languages for the same
exercise. This is achieved by using a programming language agnostic test suite.

## Installing TESTed

TESTed itself is a Python project, but you need various dependencies for the programming language specific modules. In
the readme we will only use the Python module, but which dependencies are needed for which module can be found in the
file [dependencies.md](./dependencies.md).

You need a Python 3.9 install with pip. Then execute

```bash
# Core dependencies
$ pip install -r requirements.txt
# Only needed if you want to run tests
$ pip install -r requirements-tests.txt
# Needed to evaluate submissions in Python
$ pip install -r tested/languages/python/requirements.txt
```

## Using TESTed

To use TESTed, you need an exercise with accompanying test suite. Some example exercises are available in the
folder `./exercises/`. The rest of the readme guides you through setting up a basic exercise.

### 1. Creating an exercise

We will create a simple exercise where you need to implement a function called `echo`: it receives input and must return
the same input.

Start by creating a folder for the exercise (for ease of use, we will work in this repository).

```bash
mkdir exercise/simple-example
```

### 2. Create a test suite

Next, we need a test suite. This will be used to test the submissions for the exercise. To keep things brief, we only include one test case.

```json
{
  "tabs": [{
    "name": "Echo",
    "runs": [{
      "contexts": [{
        "testcases": [{
          "input": {
            "type": "function",
            "name": "echo",
            "arguments": [{
                "type": "text",
                "data": "input-1"
            }]},
          "output": {
            "result": {
              "value": {
                "type": "text",
                "data": "input-1"
              }
            }
          }
        }]}]}]
  }]
}
```

While a little verbose, the test suite is straightforward. We have a test case that will call the function `echo` with the argument `"input-1"`. The expected return value is `"input-1"`.

You should put this file in the following location:

```bash
# Create the file
$ touch exercise/simple-example/testsuite.json
# Now you should put the content from above in the file.
```

### 3. Creating a few submissions

You should now create two submissions, a correct one and a wrong one.

```bash
$ cat exercise/simple-example/correct.py
def echo(argument):
  return argument
$ cat exercise/simple-example/wrong.py
def echo(argument):
  # Oops, this is wrong.
  return argument * 2
```

### 4. Evaluate the submissions

To run TESTed, you need to provide it with a configuration. This configuration has various options. To make things easier, save the config in the same folder. In real-world usage, this config file would be generated by whatever system TESTed is integrated.

```bash
$ cat exercise/simple-example/config.json
{
  "programming_language": "python",
  "natural_language": "en",
  "resources": "exercise/simple-example/",
  "source": "exercise/simple-example/correct.py",
  "judge": ".",
  "workdir": "workdir/",
  "plan_name": "testsuite.json",
  "memory_limit": 536870912,
  "time_limit": 60
}
```

Some information about this config:
- `programming_language`: the programming language of the submission
- `resources`: path to the folder with resources the judge can use
- `source`: path to the submission to be judged
- `judge`: path to the root of the judge source code
- `workdir`: temporary folder, see below
- `plan_name`: path to the test suite file, relative to the resources

To evaluate a submission, TESTed must generate some code. This happens in the `workdir`. Create that directory:

```bash
$ mkdir workdir/
```

This directory is kept after TESTed is run, so you can inspect what was generated. If you want to run TESTed again, you'll need to clear this directory.

Finally, you can run TESTed. The output will be printed on stdout.

```bash
$ python -m tested -c exercise/simple-example/config.json
{"command": "start-judgement"}
{"title": "Echo", "command": "start-tab"}
{"command": "start-context"}
{"description": {"description": "echo('input-1')", "format": "python"}, "command": "start-testcase"}
{"expected": "input-1", "channel": "return (String)", "command": "start-test"}
{"generated": "input-1", "status": {"enum": "correct"}, "command": "close-test"}
{"command": "close-testcase"}
{"command": "close-context"}
{"command": "close-tab"}
{"command": "close-judgement"}
```

The output is written to stdout by default. The output is the JSONLines format, meaning each JSON object will be on a different line.

All options of the command are:

```bash
$ python -m tested --help
usage: __main__.py [-h] [-c CONFIG] [-o OUTPUT] [-v]

The programming language agnostic educational test framework.

optional arguments:
  -h, --help            show this help message and exit
  -c CONFIG, --config CONFIG
                        Where to read the config from
  -o OUTPUT, --output OUTPUT
                        Where the judge output should be written to.
  -v, --verbose         Include verbose logs. It is recommended to also use -o in this case.
```

If you want to evaluate the wrong submission, you'll need to adjust the configuration.

There are some other useful commands:

```bash
# Prints the JSON Schema of the test suite format
$ python -m tested.testplan
# Run a hard-coded exercise with logs enabled, useful for debugging
$ python -m tested.manual
```

## Repo organisation

Organization of the repository:

- `exercise`: contains a series of test exercises, useful as examples and for the unit tests
- `tested`: Python project containing the code of the actual judge that will be run by Dodona
- `tests`: Unit tests for TESTed
- `benchmarking`: Some utilities to benchmark TESTed

You can run the tests with:

```bash
$ python -m pytest tests/test_functionality.py
```
